<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive SPA: Snake AI with Reinforcement Learning</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.11.0/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@3.7.0/dist/chart.min.js"></script>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f9fafb; /* Warm Neutral: bg-gray-50 */
            color: #1f2937; /* text-gray-800 */
        }
        .main-container {
            background-color: #ffffff;
            border-radius: 1rem;
            box-shadow: 0 20px 25px -5px rgb(0 0 0 / 0.1), 0 8px 10px -6px rgb(0 0 0 / 0.1);
            border: 1px solid #e5e7eb; /* border-gray-200 */
        }
        .header-title {
            color: #111827; /* text-gray-900 */
        }
        .header-subtitle {
            color: #4b5563; /* text-gray-600 */
        }
        .content-panel {
            background-color: #f9fafb; /* bg-gray-50 */
            border: 1px solid #e5e7eb; /* border-gray-200 */
            border-radius: 0.75rem; /* rounded-lg */
        }
        .chart-container {
            position: relative;
            width: 100%;
            max-width: 900px;
            margin-left: auto;
            margin-right: auto;
            height: 40vh;
            max-height: 450px;
        }
        .stat-card {
            background-color: #ffffff;
            padding: 1rem;
            border-radius: 0.75rem;
            text-align: center;
            border: 1px solid #e5e7eb;
        }
        .stat-label {
            font-size: 0.875rem;
            color: #6b7280; /* text-gray-500 */
            font-weight: 500;
        }
        .stat-value {
            font-size: 1.5rem;
            font-weight: 700;
            color: #3b82f6; /* Accent Color: Blue */
        }
        .btn {
            padding: 0.75rem 1.5rem;
            border-radius: 0.5rem;
            font-weight: 600;
            transition: all 0.2s ease-in-out;
            border: none;
            cursor: pointer;
        }
        .btn-primary {
            background-color: #3b82f6; /* bg-blue-500 */
            color: white;
        }
        .btn-primary:hover {
            background-color: #2563eb; /* bg-blue-600 */
        }
        .btn-danger {
            background-color: #ef4444; /* bg-red-500 */
            color: white;
        }
        .btn-danger:hover {
            background-color: #dc2626; /* bg-red-600 */
        }
        .section-intro {
            color: #4b5563; /* text-gray-600 */
            max-width: 48rem; /* max-w-3xl */
            margin: 0 auto 1.5rem auto; /* mx-auto mb-6 */
        }
    </style>
</head>
<body class="p-4 md:p-8">
    <!-- Chosen Palette: Warm Neutrals with Blue Accent -->
    <!-- Application Structure Plan: A dashboard-style single-page application is used to provide a holistic and interactive view of the research paper's findings. This structure was chosen over a linear, text-based format to allow users to simultaneously observe the agent's behavior (Game View), analyze its performance quantitatively (Live Stats & Training Chart), and control the simulation. This non-linear approach facilitates a deeper understanding by connecting the theoretical concepts (DQN algorithm) with their practical, observable outcomes in real-time, which is more engaging and informative than a static report. -->
    <!-- Visualization & Content Choices: 
        - Report Info: The core result of the paper - a trained DQN agent playing Snake. Goal: Demonstrate the agent's learned policy. Viz/Presentation Method: An interactive game canvas. Interaction: User can start/stop the training simulation. Justification: Directly visualizes the primary outcome of the research. Library/Method: HTML Canvas.
        - Report Info: The agent's performance over time (score per episode), as shown in Figure 5. Goal: Quantify the learning process. Viz/Presentation Method: A dynamic line chart. Interaction: The chart updates in real-time as the agent trains. Justification: Provides clear, quantitative evidence of learning, mirroring the paper's key results graph. Library/Method: Chart.js.
        - Report Info: Key training metrics like episode number and exploration rate (epsilon). Goal: Inform the user about the current state of the training process. Viz/Presentation Method: Text-based statistic cards. Interaction: Values update live. Justification: Gives context to the agent's behavior and the chart's data. Library/Method: HTML/CSS.
    -->
    <!-- CONFIRMATION: NO SVG graphics used. NO Mermaid JS used. -->

    <div class="w-full max-w-7xl mx-auto main-container p-6 md:p-8">
        <header class="text-center mb-8">
            <h1 class="text-4xl font-bold header-title">Interactive Analysis: Playing Snake with Reinforcement Learning</h1>
            <p class="text-lg header-subtitle mt-2">An interactive web application based on the research by Pan et al. Now using Double DQN.</p>
        </header>

        <main>
            <!-- Section 1: Live Simulation & Controls -->
            <section class="mb-8">
                <h2 class="text-2xl font-bold text-center mb-4">Live Simulation Environment</h2>
                <p class="text-center section-intro">
                    This section provides a real-time view of the agent playing Snake. The agent now uses a **Double Deep Q-Network (DDQN)** to learn, which helps reduce value overestimation and leads to more stable training. Click "Start Training" to observe the agent.
                </p>
                <div class="grid grid-cols-1 lg:grid-cols-3 gap-6">
                    <div class="lg:col-span-1 content-panel p-6">
                        <h3 class="text-xl font-semibold text-center mb-4">Controls</h3>
                        <div class="flex justify-center">
                            <button id="trainBtn" class="btn btn-primary w-full">Start Training</button>
                        </div>
                    </div>
                    <div class="lg:col-span-2 content-panel p-6 flex justify-center items-center">
                         <canvas id="gameCanvas" width="400" height="400"></canvas>
                    </div>
                </div>
            </section>

            <!-- Section 2: Performance Analysis -->
            <section>
                <h2 class="text-2xl font-bold text-center mb-4">Performance Analysis</h2>
                 <p class="text-center section-intro">
                    Here, you can track the agent's learning progress quantitatively. The statistics provide an instantaneous snapshot of the current training state, while the chart visualizes the agent's score over many episodes, directly corresponding to the performance graphs in the research paper.
                </p>
                <div class="content-panel p-6 mb-6">
                    <h3 class="text-xl font-semibold text-center mb-4">Live Statistics</h3>
                    <div class="grid grid-cols-2 md:grid-cols-4 gap-4">
                        <div class="stat-card">
                            <div class="stat-label">Episode</div>
                            <div id="episode-stat" class="stat-value">0</div>
                        </div>
                        <div class="stat-card">
                            <div class="stat-label">Score</div>
                            <div id="score-stat" class="stat-value">0</div>
                        </div>
                        <div class="stat-card">
                            <div class="stat-label">Total Score</div>
                            <div id="total-score-stat" class="stat-value">0</div>
                        </div>
                        <div class="stat-card">
                            <div class="stat-label">Epsilon (Explore Rate)</div>
                            <div id="epsilon-stat" class="stat-value">1.00</div>
                        </div>
                    </div>
                </div>
                <div class="content-panel p-6">
                     <h3 class="text-xl font-semibold text-center mb-4">Training Progress</h3>
                    <div class="chart-container">
                        <canvas id="scoreChart"></canvas>
                    </div>
                </div>
            </section>
        </main>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const GRID_SIZE = 20;
            const TILE_SIZE = 400 / GRID_SIZE;
            const LEARNING_RATE = 0.0005;
            const GAMMA = 0.95;
            const EPSILON_START = 1.0;
            const EPSILON_END = 0.01;
            const EPSILON_DECAY = 0.997;
            const REPLAY_BUFFER_SIZE = 10000;
            const BATCH_SIZE = 128;
            const TARGET_UPDATE_FREQUENCY = 5;
            const STATE_SIZE = 40;
            const ACTION_SIZE = 3;

            const canvas = document.getElementById('gameCanvas');
            const ctx = canvas.getContext('2d');
            const trainBtn = document.getElementById('trainBtn');
            const episodeStat = document.getElementById('episode-stat');
            const scoreStat = document.getElementById('score-stat');
            const totalScoreStat = document.getElementById('total-score-stat');
            const epsilonStat = document.getElementById('epsilon-stat');

            let isTraining = false;
            let animationFrameId;

            const chartCtx = document.getElementById('scoreChart').getContext('2d');
            const scoreChart = new Chart(chartCtx, {
                type: 'line',
                data: {
                    labels: [],
                    datasets: [{
                        label: 'Score per Episode',
                        data: [],
                        borderColor: 'rgba(59, 130, 246, 0.5)',
                        tension: 0.1,
                        pointRadius: 0,
                    }, {
                        label: 'Moving Average (50 episodes)',
                        data: [],
                        borderColor: 'rgba(239, 68, 68, 1)',
                        borderWidth: 2,
                        tension: 0.4,
                        pointRadius: 0,
                    }]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        x: { title: { display: true, text: 'Episode', color: '#4b5563' }, ticks: { color: '#6b7280' } },
                        y: { title: { display: true, text: 'Score', color: '#4b5563' }, ticks: { color: '#6b7280' } }
                    },
                    plugins: { legend: { labels: { color: '#4b5563' } } }
                }
            });

            class SnakeGame {
                constructor() { this.reset(); }
                reset() {
                    this.snake = [{ x: Math.floor(GRID_SIZE / 2), y: Math.floor(GRID_SIZE / 2) }];
                    this.direction = { x: 1, y: 0 };
                    this.food = this.generateFood();
                    this.score = 0;
                    this.gameOver = false;
                    return this.getState();
                }
                generateFood() {
                    let foodPos;
                    do {
                        foodPos = { x: Math.floor(Math.random() * GRID_SIZE), y: Math.floor(Math.random() * GRID_SIZE) };
                    } while (this.isCollidingWithSnake(foodPos));
                    return foodPos;
                }
                isCollidingWithSnake(pos) { return this.snake.some(segment => segment.x === pos.x && segment.y === pos.y); }
                step(action) {
                    const { x, y } = this.direction;
                    if (action === 1) this.direction = { x: -y, y: x };
                    else if (action === 2) this.direction = { x: y, y: -x };
                    const head = { x: this.snake[0].x + this.direction.x, y: this.snake[0].y + this.direction.y };
                    let reward = 0;
                    const oldDist = Math.abs(this.snake[0].x - this.food.x) + Math.abs(this.snake[0].y - this.food.y);
                    if (head.x < 0 || head.x >= GRID_SIZE || head.y < 0 || head.y >= GRID_SIZE || this.isCollidingWithSnake(head)) {
                        this.gameOver = true;
                        reward = -24;
                        return { state: this.getState(), reward, done: this.gameOver, score: this.score };
                    }
                    this.snake.unshift(head);
                    if (head.x === this.food.x && head.y === this.food.y) {
                        this.score++;
                        reward = 20;
                        this.food = this.generateFood();
                    } else {
                        this.snake.pop();
                        const newDist = Math.abs(head.x - this.food.x) + Math.abs(head.y - this.food.y);
                        reward = newDist < oldDist ? 0.3 : -0.5;
                    }
                    return { state: this.getState(), reward, done: this.gameOver, score: this.score };
                }
                getState() {
                    const head = this.snake[0];
                    const state = [];
                    const directions = [
                        { x: 1, y: 0 }, { x: 1, y: -1 }, { x: 0, y: -1 }, { x: -1, y: -1 },
                        { x: -1, y: 0 }, { x: -1, y: 1 }, { x: 0, y: 1 }, { x: 1, y: 1 }
                    ];
                    for (const dir of directions) {
                        let pos = { x: head.x + dir.x, y: head.y + dir.y };
                        let dist = 1;
                        while (pos.x >= 0 && pos.x < GRID_SIZE && pos.y >= 0 && pos.y < GRID_SIZE) {
                            pos.x += dir.x; pos.y += dir.y; dist++;
                        }
                        state.push(1 / dist);
                        pos = { x: head.x + dir.x, y: head.y + dir.y };
                        dist = 1;
                        let foodDist = Infinity, bodyDist = Infinity;
                        while (pos.x >= 0 && pos.x < GRID_SIZE && pos.y >= 0 && pos.y < GRID_SIZE) {
                            if (foodDist === Infinity && pos.x === this.food.x && pos.y === this.food.y) foodDist = dist;
                            if (bodyDist === Infinity && this.isCollidingWithSnake(pos)) bodyDist = dist;
                            pos.x += dir.x; pos.y += dir.y; dist++;
                        }
                        state.push(bodyDist === Infinity ? 0 : 1);
                        state.push(foodDist === Infinity ? 0 : 1);
                        state.push(1 / bodyDist);
                        state.push(1 / foodDist);
                    }
                    return state;
                }
                draw() {
                    ctx.clearRect(0, 0, canvas.width, canvas.height);
                    this.snake.forEach((segment, index) => {
                        ctx.fillStyle = index === 0 ? '#3b82f6' : '#60a5fa';
                        ctx.fillRect(segment.x * TILE_SIZE, segment.y * TILE_SIZE, TILE_SIZE, TILE_SIZE);
                    });
                    ctx.fillStyle = '#ef4444';
                    ctx.fillRect(this.food.x * TILE_SIZE, this.food.y * TILE_SIZE, TILE_SIZE, TILE_SIZE);
                }
            }

            class DQNAgent {
                constructor() {
                    this.epsilon = EPSILON_START;
                    this.replayBuffer = [];
                    this.model = this.createModel();
                    this.targetModel = this.createModel();
                    this.updateTargetModel();
                }
                createModel() {
                    const model = tf.sequential();
                    model.add(tf.layers.dense({ inputShape: [STATE_SIZE], units: 128, activation: 'relu' }));
                    model.add(tf.layers.dense({ units: 128, activation: 'relu' }));
                    model.add(tf.layers.dense({ units: ACTION_SIZE, activation: 'linear' }));
                    model.compile({ optimizer: tf.train.adam(LEARNING_RATE), loss: 'meanSquaredError' });
                    return model;
                }
                updateTargetModel() { this.targetModel.setWeights(this.model.getWeights()); }
                remember(state, action, reward, nextState, done) {
                    this.replayBuffer.push({ state, action, reward, nextState, done });
                    if (this.replayBuffer.length > REPLAY_BUFFER_SIZE) this.replayBuffer.shift();
                }
                chooseAction(state) {
                    if (Math.random() <= this.epsilon) return Math.floor(Math.random() * ACTION_SIZE);
                    return tf.tidy(() => this.model.predict(tf.tensor2d([state])).argMax(1).dataSync()[0]);
                }
                async replay() {
                    if (this.replayBuffer.length < BATCH_SIZE) return;
                    const miniBatch = Array.from({ length: BATCH_SIZE }, () => this.replayBuffer[Math.floor(Math.random() * this.replayBuffer.length)]);
                    
                    const states = miniBatch.map(e => e.state);
                    const nextStates = miniBatch.map(e => e.nextState);
                    
                    const statesTensor = tf.tensor2d(states);
                    const nextStatesTensor = tf.tensor2d(nextStates);

                    const currentQ = await this.model.predict(statesTensor).array();
                    
                    // Double DQN: Use main model to select actions, target model to evaluate them
                    const mainModelNextQ = await this.model.predict(nextStatesTensor).array();
                    const targetModelNextQ = await this.targetModel.predict(nextStatesTensor).array();

                    for (let i = 0; i < miniBatch.length; i++) {
                        const { action, reward, done } = miniBatch[i];
                        if (done) {
                            currentQ[i][action] = reward;
                        } else {
                            // Find the best action for the next state using the main model
                            const bestNextAction = mainModelNextQ[i].indexOf(Math.max(...mainModelNextQ[i]));
                            // Get the Q-value for that action from the target model
                            const targetQValue = targetModelNextQ[i][bestNextAction];
                            currentQ[i][action] = reward + GAMMA * targetQValue;
                        }
                    }

                    const newQValuesTensor = tf.tensor2d(currentQ);
                    await this.model.fit(statesTensor, newQValuesTensor, { verbose: 0 });

                    statesTensor.dispose();
                    nextStatesTensor.dispose();
                    newQValuesTensor.dispose();
                }
                decayEpsilon() { if (this.epsilon > EPSILON_END) this.epsilon *= EPSILON_DECAY; }
            }

            const game = new SnakeGame();
            const agent = new DQNAgent();
            let episode = 0;
            let totalScore = 0;
            let scores = [];

            async function runEpisode() {
                if (!isTraining) return;
                let state = game.reset();
                let done = false;
                while (!done) {
                    const action = agent.chooseAction(state);
                    const result = game.step(action);
                    agent.remember(state, action, result.reward, result.state, result.done);
                    state = result.state;
                    done = result.done;
                    game.draw();
                    scoreStat.textContent = result.score;
                    await new Promise(resolve => setTimeout(resolve, 1));
                }
                await agent.replay();
                agent.decayEpsilon();
                episode++;
                totalScore += game.score;
                scores.push(game.score);
                updateUI();
                if (episode % TARGET_UPDATE_FREQUENCY === 0) agent.updateTargetModel();
                animationFrameId = requestAnimationFrame(runEpisode);
            }

            function updateUI() {
                episodeStat.textContent = episode;
                totalScoreStat.textContent = totalScore;
                epsilonStat.textContent = agent.epsilon.toFixed(2);
                const movingAvg = scores.slice(-50).reduce((a, b) => a + b, 0) / Math.min(scores.length, 50);
                scoreChart.data.labels.push(episode);
                scoreChart.data.datasets[0].data.push(scores[scores.length - 1]);
                scoreChart.data.datasets[1].data.push(movingAvg);
                scoreChart.update();
            }

            trainBtn.addEventListener('click', () => {
                isTraining = !isTraining;
                trainBtn.textContent = isTraining ? 'Stop Training' : 'Start Training';
                trainBtn.classList.toggle('btn-primary');
                trainBtn.classList.toggle('btn-danger');
                if (isTraining) runEpisode();
                else cancelAnimationFrame(animationFrameId);
            });
            
            game.draw();
        });
    </script>
</body>
</html>
